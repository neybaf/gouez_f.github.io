[{"name":"app.R","content":"# app. R de concordancier, collocations et ngrams\ninstall.packages(c(\"shinylive\", \"httpuv\"))\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(quanteda)\nlibrary(DT)\nlibrary(udpipe)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(quanteda.textstats)\nlibrary(knitr)\nlibrary(R.temis)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(shinylive)\nlibrary(httpuv)\n# CSS pour agrandir la fenêtre\ncss <- \"\n#ngramTable, #ngramPlot, #semanticNetwork {\n  height: 1600;\n}\n.shiny-output-error-validation {\n  color: red;\n}\n\"\n\nui <- fluidPage(\n  tags$head(\n    tags$style(HTML(css))\n  ),\n  titlePanel(\"Analyse de Co-texte\"),\n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file1\", \"Choisir le corpus\",\n                multiple = TRUE,\n                accept = c(\"text/plain\", \".txt\")),\n      checkboxGroupInput(\"clean_options\", \"Nettoyer le texte\", \n                         choices = list(\"Lemmatiseur\" = \"lemma\", \n                                        \"Stopwords\" = \"stopwords\", \n                                        \"Chiffres\" = \"numbers\", \n                                        \"Symboles\" = \"symbols\", \n                                        \"Minuscules\" = \"tolower\")),\n      actionButton(\"clean_text\", \"Nettoyer le texte\"),\n      radioButtons(\"analysis_type\", \"Type d'analyse\", choices = c(\"Concordance\", \"Co-occurrences\", \"Ngrams\")),\n      conditionalPanel(\n        condition = \"input.analysis_type == 'Concordance'\",\n        textInput(\"n_words\", \"Co-texte:\", value = 5), \n        textInput(\"myterm\", \"Mot clé:\", value = \"nation\")\n      ),\n      conditionalPanel(\n        condition = \"input.analysis_type == 'Co-occurrences'\",\n        textInput(\"myterm\", \"Mot clé:\", value = \"texte\")\n      ),\n      actionButton(\"update\", \"Mettre à jour\"),\n      downloadButton(\"downloadData\", \"Télécharger les données\")\n    ),\n    mainPanel(\n      DTOutput(\"ngramTable\"),\n      uiOutput(\"cooccurrenceTable\"),\n      plotlyOutput(\"ngramPlot\", height = \"700px\") # Ajout de height pour agrandir le graphique\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  raw_corpus_data <- reactiveVal(NULL)\n  cleaned_corpus_data <- reactiveVal(NULL)\n  \n  observeEvent(input$file1, {\n    req(input$file1)\n    \n    # Charger les données et créer le corpus combiné\n    combine_files <- function(filepaths) {\n      texts <- lapply(filepaths, readLines, encoding = \"UTF-8\")\n      combined_text <- unlist(texts)\n      return(combined_text)\n    }\n    \n    files_comb <- input$file1$datapath\n    combined_text <- combine_files(files_comb)\n    raw_corpus_data(combined_text)\n    cleaned_corpus_data(combined_text) # Initialiser avec les textes bruts\n  })\n  \n  observeEvent(input$clean_text, {\n    req(raw_corpus_data())\n    \n    combined_text <- raw_corpus_data() # Utiliser les textes bruts pour chaque nettoyage\n    \n    # Nettoyage du texte selon les options sélectionnées\n    tokens_comb <- tokens(combined_text)\n    if (\"lemma\" %in% input$clean_options) {\n      # Charger le modèle UDPipe\n      ud_model <- udpipe_load_model(\"/Users/fabiengouez/Library/Mobile Documents/com~apple~CloudDocs/Downloads/french-gsd-ud-2.5-191206(1).udpipe\")\n      \n      # Fonction pour annoter et lemmatiser un document\n      annotate_and_lemmatize <- function(text, ud_model) {\n        annotations <- udpipe_annotate(ud_model, x = text)\n        annotations <- as.data.frame(annotations)\n        annotations$lemma <- ifelse(is.na(annotations$lemma) | annotations$lemma == \"\", annotations$token, annotations$lemma)\n        modified_sentence <- paste(annotations$lemma, collapse = \" \")\n        return(modified_sentence)\n      }\n      \n      # Assurer la correspondance des longueurs pour udpipe_annotate\n      lemmatized_texts <- sapply(seq_along(combined_text), function(i) {\n        text <- combined_text[i]\n        annotate_and_lemmatize(text, ud_model)\n      })\n      \n      # Recréer le corpus avec les textes lemmatisés\n      corpus_comb <- corpus(lemmatized_texts, docnames = paste0(\"doc\", seq_along(lemmatized_texts)))\n      tokens_comb <- tokens(corpus_comb)\n    }\n    if (\"stopwords\" %in% input$clean_options) {\n      tokens_comb <- tokens_remove(tokens_comb, pattern = stopwords(\"fr\"))\n    }\n    if (\"numbers\" %in% input$clean_options) {\n      tokens_comb <- tokens_remove(tokens_comb, pattern = \"\\\\d+\", valuetype = \"regex\")\n    }\n    if (\"symbols\" %in% input$clean_options) {\n      tokens_comb <- tokens_keep(tokens_comb, pattern = \"[\\\\p{L}\\\\p{N}]+\", valuetype = \"regex\")\n    }\n    if (\"tolower\" %in% input$clean_options) {\n      tokens_comb <- tokens_tolower(tokens_comb)\n    }\n    cleaned_corpus_data(tokens_comb)\n  })\n  \n  observeEvent(input$update, {\n    req(cleaned_corpus_data())\n    \n    if (input$analysis_type == \"Concordance\") {\n      extract_concordance <- function(doc, term, n_words) {\n        words <- unlist(strsplit(doc, \"\\\\s+\"))\n        term_positions <- which(words == term)\n        concordances <- sapply(term_positions, function(pos) {\n          start <- max(1, pos - n_words)\n          end <- min(length(words), pos + n_words)\n          paste(words[start:end], collapse = \" \")\n        })\n        return(concordances)\n      }\n      \n      tokens_comb <- cleaned_corpus_data()\n      \n      concordances_list <- lapply(tokens_comb, function(doc) {\n        extract_concordance(as.character(doc), input$myterm, as.numeric(input$n_words))\n      })\n      \n      concordances_df <- data.frame(text = unlist(concordances_list))\n      \n      # Tableau interactif\n      output$ngramTable <- renderDT({\n        datatable(concordances_df)\n      })\n      \n      # Télécharger les données\n      output$downloadData <- downloadHandler(\n        filename = function() {\n          paste(\"ngram_data-\", Sys.Date(), \".csv\", sep = \"\")\n        },\n        content = function(file) {\n          write.csv(concordances_df, file, row.names = FALSE)\n        }\n      )\n      \n      # Cacher le tableau de co-occurrences\n      output$cooccurrenceTable <- renderUI(NULL)\n      output$ngramPlot <- renderPlotly(NULL)\n    }  else if (input$analysis_type == \"Co-occurrences\") {\n      req(cleaned_corpus_data())\n      \n      # Créer une matrice terme-document\n      dtm <- dfm(tokens(cleaned_corpus_data()))\n      \n      # Calculer les co-occurrences avec le terme spécifié\n      term_fcm <- fcm(dtm)\n      \n      ## Vérifier si le terme spécifié est présent dans la matrice\n      if (!(input$myterm %in% colnames(term_fcm))) {\n        cooc_df <- data.frame(message = \"No co-occurrences found for the specified term.\")\n      } else {\n        # Extraire la colonne des co-occurrences pour le terme spécifié\n        cooc_vector <- as.vector(term_fcm[, input$myterm])\n        cooc_terms <- colnames(term_fcm)\n        \n        # Créer un DataFrame à partir du vecteur de co-occurrences\n        cooc_df <- data.frame(Term = cooc_terms, Cooccurrences = cooc_vector)\n        \n        # Filtrer les termes avec au moins une co-occurrence\n        cooc_df <- cooc_df[cooc_df$Cooccurrences > 5, ]\n        \n        # Vérifier si des co-occurrences ont été trouvées après filtrage\n        if (nrow(cooc_df) == 0) {\n          cooc_df <- data.frame(message = \"No co-occurrences found for the specified term.\")\n        } else {\n          # Trier le DataFrame par nombre de co-occurrences, décroissant\n          cooc_df <- cooc_df[order(cooc_df$Cooccurrences, decreasing = TRUE), ]\n        }\n      }\n      \n      # Générer le titre et la légende du tableau\n      title <- paste(\"Co-occurrences de:\", input$myterm)\n      # Générer le tableau et l'afficher\n      output$cooccurrenceTable <- renderUI({\n        x1 <- knitr::kable(cooc_df, caption = title, format = \"html\")\n        HTML(x1)\n      })\n      \n      # Cacher le tableau de concordance\n      output$ngramTable <- renderDT(NULL)\n      output$ngramPlot <- renderPlotly(NULL)\n    }\n    else if (input$analysis_type == \"Ngrams\") {\n      req(cleaned_corpus_data())\n      \n      # Obtenir les tokens à partir des données nettoyées\n      tokens_comb <- tokens(cleaned_corpus_data())\n      \n      # Calculer les collocations (ngrams) avec quanteda.textstats\n      collocations <- textstat_collocations(tokens_comb, size = 2:5)\n      \n      # Filtrer les collocations pour n'afficher que les plus fréquentes\n      collocations <- collocations[order(collocations$count, decreasing = TRUE), ]\n      \n      # Limiter le nombre de collocations affichées (par exemple, les 20 premières)\n      top_collocations <- head(collocations, 50)\n      \n      p <- ggplot(top_collocations, aes(x = reorder(collocation, count), y = count)) +\n        geom_bar(stat = \"identity\", fill = \"steelblue\") +\n        coord_flip() +\n        labs(\n          title = \"Top 20 Collocations (2-3 words)\",\n          x = \"Collocations\",\n          y = \"Frequency\"\n        ) +\n        theme_minimal()\n      \n      # Convertir le graphique ggplot en un graphique interactif avec plotly et ajouter des annotations\n      output$ngramPlot <- renderPlotly({\n        ggplotly(p) \n      })\n      # Cacher les autres tableaux\n      output$ngramTable <- renderDT(NULL)\n      output$cooccurrenceTable <- renderUI(NULL)\n    }\n  })\n}\n\nshinyApp(ui, server)\nshinylive::export(appdir = \"/Users/fabiengouez/Desktop/dossier sans titre\", destdir = \"docs\")\n","type":"text"}]
